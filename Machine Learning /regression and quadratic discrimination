#Load the necessary libraries
library(MASS) # For QDA
library(ggplot2) # For plotting
library(dplyr) # For data manipulation
library(tidyr) # For data manipulation



# Load the dataset from URL
df <- read.csv("https://raw.githubusercontent.com/JaySquare87/DTA395-1Rollins/main/CodeProjects/RPG.csv", header = TRUE, sep = ",")

# Code Project Starts Here.

#==================
# LINEAR REGRESSION
#==================
# manualy calculate lm() to predict LEVEL from Armor, Weapon, Physical, and Magic
RPGdfX <- df |> select(Armor, Weapon, Physical, Magic)
RPGdfX <- cbind(1, RPGdfX)
RPGdfX <- as.matrix(RPGdfX)

beta <- solve(t(RPGdfX) %*% RPGdfX) %*% t(RPGdfX) %*% df$Level
y_hat <- RPGdfX %*% beta
df <- df |> mutate(level_pred = y_hat)

# lm() to check accuracy
multi_fit <- lm(Level ~ Armor + Weapon + Physical + Magic, data = df)
summary(multi_fit)

# calculate accuracy


# split data to training (70%) and testing (30%)
set.seed(123)
train_data <- sample(1:nrow(df), 0.7 * nrow(df))
train <- df[train_data, ]
test <- df[-train_data, ]


# fit model with training subset manually




# calculate accuracy

# do that thing that the teacher said in class but no one understood

#====================
# LOGISTIC REGRESSION
#====================
# predict weather FBOSS is beat using WEAPON and MAGIC
multiple_logistic_regression <- function(beta, X, y) 
{
  z <- X %*% beta
  p <- exp(z) / (1 + exp(z))
  likelihood <- -sum(log(y * p + (1 - y) * (1 - p)))
  return(likelihood)
}

df$intercept <- 1
X <- as.matrix(df[, c('intercept', 'Weapon', 'Magic')])
y <- df$FBoss

initial_betas <- rep(0, ncol(X))
# something wrong with optim_betas
optimized_betas <- optim(initial_betas, multiple_logistic_regression, X = X, y = y, control = list(maxit = 1000))
optimized_betas

# verify using glm
glm.fit <- glm(FBoss ~ Weapon + Magic, data = df, family = binomial)
summary(glm.fit)

# calculate accuracy

# use k-fold validation to verify accuracy





#=========================
# QUADRATIC DISCRIMINATION
#=========================
#Laura did this section. Forgive her if this is absolute bs. 
# Subset the data for QDA model
qda_data <- data[, c("Magic", "Weapon", "Class")]

# Create the QDA model
qda_model_magic_weapon <- qda(Class ~ Magic + Weapon, data = qda_data)

# Summary of the model
summary(qda_model_magic_weapon)
# Create a grid
grid_data <- expand.grid(Magic = seq(min(data$Magic), max(data$Magic), length.out = 100),
                         Weapon = seq(min(data$Weapon), max(data$Weapon), length.out = 100))

# Predict class membership for grid
grid_data$Class <- predict(qda_model_magic_weapon, newdata = grid_data)$class

# Base scatter plot
p <- ggplot(data, aes(x = Magic, y = Weapon, color = Class)) +
  geom_point() +
  theme_minimal()

# Add the grid as another layer
p + geom_tile(data = grid_data, aes(x = Magic, y = Weapon, fill = Class), alpha = 0.2) +
  scale_fill_manual(values = c("red", "blue", "green")) # Adjust colors as needed
# Create the QDA model with all variables
qda_model_all <- qda(Class ~ Armor + Weapon + Physical + Magic, data = data)

# Summary of the model
summary(qda_model_all)
# Splitting the data (example: 70% training, 30% testing)
set.seed(123) # For reproducibility
training_indices <- sample(1:nrow(data), 0.7 * nrow(data))
training_data <- data[training_indices, ]
testing_data <- data[-training_indices, ]

# Fit the model on training data
qda_model_all_train <- qda(Class ~ Armor + Weapon + Physical + Magic, data = training_data)

# Predict on testing data
predictions <- predict(qda_model_all_train, newdata = testing_data)$class

# Calculate accuracy
accuracy <- mean(predictions == testing_data$Class)
print(paste("Accuracy:", accuracy))


